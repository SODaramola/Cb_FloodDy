{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e1ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U Cb-FloodDy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c350bd",
   "metadata": {},
   "source": [
    "## Voronoi Workflow Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d55d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyproj import CRS\n",
    "import matplotlib.pyplot as plt\n",
    "from Cb_FloodDy import voronoi_clusters\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "# Define source CRS (WGS84)\n",
    "src_crs = CRS.from_epsg(4326)\n",
    "\n",
    "# Custom ticks for longitude and latitude\n",
    "longitudes = [-95.5, -95.0, -94.5]\n",
    "latitudes = [29.0, 29.4, 29.8]\n",
    "\n",
    "# Range of station files to include (inclusive)\n",
    "start_idx, end_idx = 1, 21  # e.g., station_1.csv through station_21.csv\n",
    "\n",
    "# Directory containing station CSV files\n",
    "station_dir = 'training_water_level'\n",
    "\n",
    "# Load the floodmap shapefile\n",
    "shapefile_path = 'GBay_cells_polygon.shp'\n",
    "\n",
    "# Whether to combine polygons and which to combine (1-based indices)\n",
    "# combine_pairs = [(1, 19), (12, 21), (3, 18)]  # set [] or None to skip combining\n",
    "\n",
    "# Whether to reorder polygons so index matches station number\n",
    "reorder_by_station = True\n",
    "\n",
    "# Output paths (change as needed)\n",
    "out_shapefile = 'voronoi_clusters.shp'\n",
    "out_fig = os.path.join('figures', 'voronoi_map.png')\n",
    "\n",
    "summary = voronoi_clusters.run_workflow(\n",
    "    src_crs=src_crs,\n",
    "    station_dir=station_dir,\n",
    "    station_range=(start_idx, end_idx),\n",
    "    shapefile_path=shapefile_path,\n",
    "    x_ticks=longitudes,\n",
    "    y_ticks=latitudes,\n",
    "    out_shapefile=out_shapefile,\n",
    "    out_fig=out_fig,\n",
    "    reorder_by_station=True,\n",
    "#     lon_name=\"Longitude_dd\",   # optional if auto-detect fails\n",
    "#     lat_name=\"Latitude_dd\",    # optional if auto-detect fails\n",
    ")\n",
    "\n",
    "summary  # show a summary dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81730d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bbd5b6f",
   "metadata": {},
   "source": [
    "## Bayesian Optimization Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730cae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cb_FloodDy import bayesian_opt_tuning as bo\n",
    "import os\n",
    "\n",
    "# === Data directories ===\n",
    "train_atm_pressure_dir = os.path.join(os.getcwd(), 'atm_pressure')\n",
    "train_wind_speed_dir   = os.path.join(os.getcwd(), 'wind_speed')\n",
    "train_precipitation_dir= os.path.join(os.getcwd(), 'precipitation')\n",
    "train_water_depth_dir  = os.path.join(os.getcwd(), 'water_depth')\n",
    "train_river_discharge_dir = os.path.join(os.getcwd(), 'river_discharge')\n",
    "water_level_dir = os.path.join(os.getcwd(), 'training_water_level')\n",
    "polygon_clusters_path  = os.path.join(os.getcwd(), 'voronoi_clusters.shp')\n",
    "\n",
    "# === DEMs (explicit lengths per DEM; must sum to total T) ===\n",
    "dem_files = [\n",
    "    os.path.join(os.getcwd(), 'DEM/dem_1.tif'),\n",
    "    os.path.join(os.getcwd(), 'DEM/dem_2.tif'),\n",
    "#     os.path.join(os.getcwd(), 'DEM/dem_3.tif'),\n",
    "#     os.path.join(os.getcwd(), 'DEM/dem_4.tif'),\n",
    "]\n",
    "dem_timesteps = [217, 410]  # first 217 use DEM1, next 100 use DEM2, next 300 DEM3, next 151 DEM4\n",
    "\n",
    "# === Temporal ===\n",
    "sequence_length = 6\n",
    "\n",
    "# === Hyperparameter spaces ===\n",
    "convlstm_filters = [16, 32, 48, 64]\n",
    "lstm_units       = [32, 48, 64]\n",
    "dense_units      = [32, 48, 64]\n",
    "l2_reg_range     = (1e-6, 1e-3)\n",
    "lr_range         = (1e-5, 1e-3)\n",
    "dropout_range    = (0.2, 0.5)\n",
    "\n",
    "# === BO / training ===\n",
    "n_trials   = 2\n",
    "epochs     = 10\n",
    "batch_size = 2\n",
    "val_split  = 0.2\n",
    "checkpoint_dir_BO = \"checkpoint_BO\"\n",
    "study_name = \"Flood_Depth_Prediction_BO\"\n",
    "\n",
    "# === Seed & EarlyStopping ===\n",
    "seed_value = 42\n",
    "es_monitor = \"val_loss\"\n",
    "early_stopping = 5\n",
    "es_restore_best = True\n",
    "\n",
    "summary = bo.run_optimization(\n",
    "    train_atm_pressure_dir=train_atm_pressure_dir,\n",
    "    train_wind_speed_dir=train_wind_speed_dir,\n",
    "    train_precipitation_dir=train_precipitation_dir,\n",
    "    train_water_depth_dir=train_water_depth_dir,\n",
    "    train_river_discharge_dir=train_river_discharge_dir,\n",
    "    water_level_dir=water_level_dir,\n",
    "    polygon_clusters_path=polygon_clusters_path,\n",
    "    sequence_length=sequence_length,\n",
    "    n_trials=n_trials,\n",
    "    study_name=study_name,\n",
    "    checkpoint_dir_BO=checkpoint_dir_BO,\n",
    "    seed_value=seed_value,\n",
    "    convlstm_filters=convlstm_filters,\n",
    "    lstm_units=lstm_units,\n",
    "    dense_units=dense_units,\n",
    "    l2_reg_range=l2_reg_range,\n",
    "    lr_range=lr_range,\n",
    "    dropout_range=dropout_range,\n",
    "    es_monitor=es_monitor,\n",
    "    early_stopping=early_stopping,\n",
    "    es_restore_best=es_restore_best,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    val_split=val_split,\n",
    "    # Dynamic multi-DEM\n",
    "    dem_files=dem_files,\n",
    "    dem_timesteps=dem_timesteps,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df2f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d58e7b5b",
   "metadata": {},
   "source": [
    "## Flood Depth Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18812074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cb_FloodDy import model_prediction as mp\n",
    "import os\n",
    "\n",
    "# === TEST/EVAL SETTINGS ===\n",
    "checkpoint_dir_BO     = \"checkpoint_BO\"  # folder with trial_*/ and artifacts/\n",
    "polygon_clusters_path = os.path.join(os.getcwd(), 'voronoi_clusters.shp')  # fallback if cluster_masks.npy missing\n",
    "sequence_length       = 6\n",
    "seed_value            = 42\n",
    "\n",
    "# this MUST match your actual folder name on disk\n",
    "test_name = \"testingharvey\"\n",
    "\n",
    "# everything lives under this directory:\n",
    "base_test_dir = os.path.join(os.getcwd(), test_name)\n",
    "\n",
    "mp.run_predictions(\n",
    "    # Input rasters (TIFFs), all inside testingharvey/\n",
    "    test_atm_pressure_dir    = os.path.join(base_test_dir, 'atm_pressure'),\n",
    "    test_wind_speed_dir      = os.path.join(base_test_dir, 'wind_speed'),\n",
    "    test_precipitation_dir   = os.path.join(base_test_dir, 'precipitation'),\n",
    "    test_river_discharge_dir = os.path.join(base_test_dir, 'river_discharge'),\n",
    "    test_water_depth_dir     = os.path.join(base_test_dir, 'water_depth'),\n",
    "\n",
    "    # Water level CSVs are in testingharvey/original_water_level\n",
    "    test_water_level_dir     = os.path.join(base_test_dir, 'original_water_level'),\n",
    "\n",
    "    # DEM used for testing (single raster broadcast across timesteps)\n",
    "    test_dem_file            = os.path.join(base_test_dir, 'DEM', 'dem_idw.tif'),\n",
    "\n",
    "    # BO output from training\n",
    "    checkpoint_dir_BO        = checkpoint_dir_BO,\n",
    "\n",
    "    # Cluster polygons (only used if cluster_masks.npy wasn't saved in artifacts/)\n",
    "    polygon_clusters_path    = polygon_clusters_path,\n",
    "\n",
    "    # Temporal setup\n",
    "    sequence_length          = sequence_length,\n",
    "\n",
    "    # Output layout\n",
    "    output_root              = \"predictions\",    # parent folder where results go\n",
    "    test_name                = test_name,        # this becomes predictions/testingharvey/...\n",
    "\n",
    "    # Reproducibility\n",
    "    seed_value               = seed_value,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d75d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
